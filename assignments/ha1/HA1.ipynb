{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDDA. Home Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you need to implement your version of the PCA in case you chose the base level, and SparcePCA in case you are a champion. \n",
    "\n",
    "- The task consists of three stages. At the first stage, you are invited to demonstrate PCA on a classic dataset. You need to work with and explain why there is a loss of information.\n",
    "\n",
    "- In the next step, you need to implement an algorithm that matches the level you selected (basic = PCA, champion = SparcePCA), and to test your implementation on a simple synthetic example in which the main components are well identified.\n",
    "\n",
    "- The third stage is a creative task. You are offered a dataset from the kaggle contest. You are required to solve this task, provide your decisions with comments on why one or another step was taken. After the deadline, we will review your decisions in a discussion format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin with, we consider the demonstration case of classifying sets that are well separable. We use the function of generating multidimensional blobs from the `sklearn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples = 10000, n_features = 5, random_state = 101, centers=4,\n",
    "                  cluster_std=[5.1, 4.2, 5.3, 2.4],\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the resulting set by using `pyplot` from `matplotlib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], s=10, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gloom..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay. Let's see the projections on the first two axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a color map\n",
    "color_map = dict(zip([0,1,2,3], [\"red\", \"blue\", \"green\", \"yellow\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw original set by using simple for comprehension\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "{plt.scatter(X[y == k, 0], X[y == k, 1], c=v, s=10, edgecolor='k') for (k,v) in color_map.items()}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice! Now let's see what we have with clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a 4 KMeans clustering\n",
    "kmeans_test = KMeans(n_clusters = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute cluster centers and predict cluster indices\n",
    "X_test = kmeans_test.fit_predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# make a color map for predicted cluster indices\n",
    "kmeans_color_map = dict(zip(np.unique(kmeans_test.labels_), [\"red\", \"blue\", \"green\", \"yellow\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw clustered set\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "{plt.scatter(X[y == k, 0], X[y == k, 1], c=v, s=10, edgecolor='k') for (k,v) in kmeans_color_map.items()} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! What about clustering metrics? Let's see all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ARI: '+str(metrics.adjusted_rand_score(y, kmeans_test.labels_)))\n",
    "print('AMI: '+str(metrics.adjusted_mutual_info_score(y, kmeans_test.labels_)))\n",
    "print('Homogenity: '+str(metrics.homogeneity_score(y, kmeans_test.labels_)))\n",
    "print('Completeness: '+str(metrics.completeness_score(y, kmeans_test.labels_)))\n",
    "print('V-measure: '+str(metrics.v_measure_score(y, kmeans_test.labels_)))\n",
    "print('Silhouette: '+str(metrics.silhouette_score(X, kmeans_test.labels_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more info you can use [7 topic of ODS Course](https://www.kaggle.com/kashnitsky/topic-7-unsupervised-learning-pca-and-clustering).\n",
    "\n",
    "Russian version of the article [here](https://habr.com/company/ods/blog/325654/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check how much components have influence in terms of explained variance. The [article](http://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#explained-variance) can help you to realize the concept of explained variance. Here is the support function below: for plotting ` explained variance/principle components` flat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# survice function for Explained variance ratio plotting\n",
    "def plot_explained_variance(X):\n",
    "    #Calculating Eigenvecors and eigenvalues of Covariance matrix\n",
    "    mean_vec = np.mean(X, axis=0)\n",
    "    cov_mat = np.cov(X.T)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "    # Create a list of (eigenvalue, eigenvector) tuples\n",
    "    eig_pairs = [ (np.abs(eig_vals[i]),eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "    # Sort from high to low\n",
    "    eig_pairs.sort(key = lambda x: x[0], reverse= True)\n",
    "\n",
    "    # Calculation of Explained Variance from the eigenvalues\n",
    "    tot = sum(eig_vals)\n",
    "    var_exp = [(i/tot)*100 for i in sorted(eig_vals, reverse=True)] # Individual explained variance\n",
    "    cum_var_exp = np.cumsum(var_exp) # Cumulative explained variance\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(len(var_exp)), var_exp, alpha=0.3333, align='center', label='individual explained variance', color = 'g')\n",
    "    plt.step(range(len(cum_var_exp)), cum_var_exp, where='mid',label='cumulative explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use it properly it would be better to use [Feature Scaling](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html)\n",
    "by using `StandardScaler` from the `sklearn` preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = StandardScaler().fit_transform(X)\n",
    "plot_explained_variance(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above clearly shows that most of the variance can be explained by the 4 first principal components. But to see more in detail, let's look at the correlation matrix. To do this, we first wrap the data in the `DataFrame` (`pandas` module) and then use the `heatmap` from `seaborn` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['feature_' + str(x) for x in np.arange(1, 7, 1)]\n",
    "d = {key: values for key, values in zip(columns, X.T)}\n",
    "data = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(12,10))\n",
    "plt.title('Pearson Correlation of Movie Features')\n",
    "sns.heatmap(data.astype(float).corr(), linewidths=0.25, vmax=1.0, square=True,\n",
    "           cmap=\"YlGnBu\", linecolor='black', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, it's time to use PCA. Let's try it with 4 components and and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "X_transformed = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw transformed set using simple for comprehension\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "{plt.scatter(X_transformed[y == k, 0], X_transformed[y == k, 1], c=v, s=10, edgecolor='k') for (k,v) in color_map.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set a 4 KMeans clustering\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "\n",
    "#Compute cluster centers and predict cluster indices\n",
    "X_clustered = kmeans.fit_predict(X_transformed)\n",
    "\n",
    "print('ARI: '+str(metrics.adjusted_rand_score(y, kmeans.labels_)))\n",
    "print('AMI: '+str(metrics.adjusted_mutual_info_score(y, kmeans.labels_)))\n",
    "print('Homogenity: '+str(metrics.homogeneity_score(y, kmeans.labels_)))\n",
    "print('Completeness: '+str(metrics.completeness_score(y, kmeans.labels_)))\n",
    "print('V-measure: '+str(metrics.v_measure_score(y, kmeans.labels_)))\n",
    "print('Silhouette: '+str(metrics.silhouette_score(X, kmeans.labels_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that there was a loss of information and the quality of clustering has greatly decreased. Try to explain this fact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Explain why in the example above there was a loss of information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSER HERE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: A Synthetic Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A synthetic example contains 10 features, each of which is a simple linear combination of three values: $V1, V2, V3$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$V_1 \\sim N(0, 290)$$\n",
    "$$V_2 \\sim N (0, 300)$$\n",
    "$$V_3 = −0.3 V_1 + 0.925 V_2 + \\epsilon, \\ \\epsilon \\sim N(0, 1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to find out how much of this data set has the main components, and compare the speed of your PCA implementation (SPCA) and the implementation of the library version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first column contains numbers, so we'll drop it away\n",
    "\n",
    "df = pd.read_csv(\"./synthetic.csv\").drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not forget to use the scaling\n",
    "X_std = StandardScaler().fit_transform(df.values)\n",
    "\n",
    "# Determine the number of main components.\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the covariation matrix by using sns.heatmap(...) (optionally)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test your hypothesis and measure the running time of the library versions of the algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import SparsePCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=???)\n",
    "x_d = pca.fit_transform(X_std)\n",
    "plt.scatter(x_d[:,0],x_d[:,1], s=10, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spca = SparsePCA(n_components=???)\n",
    "x_d = spca.fit_transform(X_std)\n",
    "plt.scatter(x_d[:,0],x_d[:,1], s=10, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit spca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Implement you version of PCA (or SPCA) and using template above check it time performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR IMPLEMENTATION OF PCA (FOR NOVICES)\n",
    "class myPCA():\n",
    "    def __init__(self, n_components=None):\n",
    "        # YOUR CODE HERE\n",
    "    def fit_transform(self, X, y=None):\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = myPCA(n_components=???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST TIME PERFORMANCE OF YOUR PCA\n",
    "%timeit pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAW RESULTS\n",
    "x_d = pca.fit_transform(X_std)\n",
    "plt.scatter(x_d[:,0],x_d[:,1], s=10, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SparcePCA (for champions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR IMPLEMENTATION OF SparcePCA (FOR CHAMPIONS)\n",
    "class mySparcePCA():\n",
    "    def __init__(self, n_components=None):\n",
    "        # YOUR CODE HERE\n",
    "    def fit_transform(self, X, y=None):\n",
    "        # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spca = mySparcePCA(n_components=???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST TIME PERFORMANCE OF YOUR SPCA\n",
    "%timeit spca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRAW RESULTS\n",
    "x_d = spca.fit_transform(X_std)\n",
    "plt.scatter(x_d[:,0],x_d[:,1], s=10, edgecolor='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Сreative task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I offer your to have a practive with your implementation on [TMDB 5000 Movie Dataset](https://www.kaggle.com/tmdb/tmdb-movie-metadata) Kaggle dataset. \n",
    "\n",
    "You need to identify the number of main components and make clustering using Kmeans.\n",
    "\n",
    "- Preliminary preparation of the dataset: get rid of zero values, transform categorical features.\n",
    "\n",
    "- Scale the dataset (use StandardScaler or anything you want).\n",
    "\n",
    "- Identify the number of main components (demonstrate the analysis toolkit).\n",
    "\n",
    "- Reduce the dimensionality of the space by using your PCA implementation (SPCA).\n",
    "\n",
    "- Cluster the dataset by Kmeans algo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import SparsePCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./tmdb_5000_movies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
